{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Logitboost Assignment\n",
    "Implement Logitboost using 1D linear regressors as weak learners. At each boosting\n",
    "iteration choose the weak learner that obtains the largest reduction in the loss function\n",
    "on the training set $D = {(x_i, y_i), i = 1, ..., N}, \\text{ with } y_i ∈ {0, 1}$:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data loading functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_gisette():\n",
    "    path = \"data/gisette/\"   \n",
    "    \n",
    "    train_x = np.loadtxt(path+\"gisette_train.data\")\n",
    "    train_y = np.loadtxt(path+\"gisette_train.labels\")\n",
    "    \n",
    "    valid_x = np.loadtxt(path+\"gisette_valid.data\")\n",
    "    valid_y = np.loadtxt(path+\"gisette_valid.labels\")\n",
    "    \n",
    "    return train_x, train_y, valid_x, valid_y\n",
    "\n",
    "\n",
    "\n",
    "def get_dexter():\n",
    "    path = \"data/dexter/\"\n",
    "\n",
    "    train_x = np.loadtxt(path+\"dexter_train.csv\", delimiter=',')\n",
    "    train_y = np.loadtxt(path+\"dexter_train.labels\")\n",
    "\n",
    "    valid_x = np.loadtxt(path+\"dexter_valid.csv\", delimiter=',')\n",
    "    valid_y = np.loadtxt(path+\"dexter_valid.labels\")\n",
    "\n",
    "    return train_x, train_y, valid_x, valid_y\n",
    "\n",
    "def get_madelon():\n",
    "    path = \"data/MADELON/\"\n",
    "\n",
    "    train_x = np.loadtxt(path + \"madelon_train.data\")\n",
    "    train_y = np.loadtxt(path + \"madelon_train.labels\")\n",
    "    test_x = np.loadtxt(path + \"madelon_valid.data\")\n",
    "    test_y = np.loadtxt(path + \"madelon_valid.labels\")\n",
    "\n",
    "    return train_x, train_y, test_x, test_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Normalization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-3-6e6b330f19b9>, line 58)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;36m  File \u001B[1;32m\"<ipython-input-3-6e6b330f19b9>\"\u001B[1;36m, line \u001B[1;32m58\u001B[0m\n\u001B[1;33m    print(\"nonzero\", np.sum(np.nonzero(beta_1)))\u001B[0m\n\u001B[1;37m    ^\u001B[0m\n\u001B[1;31mIndentationError\u001B[0m\u001B[1;31m:\u001B[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "def normalize(train, *args):\n",
    "    mean = np.average(train, axis=0)\n",
    "    standard_deviation = np.std(train, axis=0)\n",
    "    columns = train, *args\n",
    "    return tuple(np.divide(column-mean, standard_deviation, where=standard_deviation!=0)\n",
    "                 for column in columns)\n",
    "\n",
    "\n",
    "def beta_selection(x, beta0, beta1, y, h):\n",
    "\n",
    "    #min_loss = np.inf\n",
    "    #min_index = None\n",
    "    #for i, b in enumerate(np.column_stack(np.dstack((beta0,beta1)))):\n",
    "        #h2 = np.copy(h)\n",
    "        #h2[i]+=b\n",
    "       ## if np.sum(h[i]) != 0:\n",
    "         ##print(\"after\", all_betas[i,i])\n",
    "        ##all_betas[i, :]=h2\n",
    "        #prediction = np.sum(x*h2[:,1] + h2[:,0], axis=1)\n",
    "        #loss=np.sum(np.log(1+np.exp(-ytilde*prediction.T)))\n",
    "        #if loss < min_loss:\n",
    "            #min_loss = loss\n",
    "            #min_index=i\n",
    "    h_prediction = np.sum(x * h[:,1] + h[:,0], axis=1)\n",
    "    new_beta1s = np.diag(beta1) # create diagonal p x p matrix\n",
    "    beta_predictions = np.dot(x,new_beta1s) + beta0\n",
    "    #print(h_prediction.shape)\n",
    "    #print(beta_predictions.shape)\n",
    "    final_prediction = beta_predictions+h_prediction[np.newaxis].T\n",
    "    #print(final_prediction.shape)\n",
    "    ytilde=2*y-1\n",
    "    loss=np.sum(np.log(1+np.exp(-ytilde*final_prediction.T)), axis=1)\n",
    "    print(\"loss min\", np.min(loss))\n",
    "    print(\"loss max\", np.max(loss))\n",
    "\n",
    "    beta_index = np.argmin(loss)\n",
    "    filter = np.ones_like(beta0) # array of ones of same length as beta0\n",
    "    filter[beta_index] = 0 # set the spot at beta_index to 0\n",
    "    beta0[filter==1] = 0 # all indices at which filter==1 are set to 0, to beta 0 has zeroes everywhere except beta_index\n",
    "    beta1[filter==1] = 0\n",
    "    return np.column_stack(np.dstack((beta0,beta1)))\n",
    "\n",
    "\n",
    "\n",
    "#def h()\n",
    "# I separated linear regression to its own function and transposed to vectors so they\n",
    "# broadcast correctly\n",
    "def linear_regressor_for_each_feature(x, y, w):\n",
    "\n",
    "    mean_x = np.sum(x*w[np.newaxis].T, axis=0)/np.sum(w) # add an axis so we can tranpose this\n",
    "    mean_y = np.sum(w*y)/np.sum(w)\n",
    "    denominators = w @ np.square(x-mean_x)\n",
    "    #print(\"denominator\", denominators.shape)\n",
    "    beta_1= w*(y-mean_y)@(x-mean_x)/ denominators\n",
    "    beta_0 = mean_y + beta_1 * mean_x\n",
    "    beta_0[np.isnan(beta_0)] = 0\n",
    "    beta_1[np.isnan(beta_1)] = 0\n",
    "        print(\"nonzero\", np.sum(np.nonzero(beta_1)))\n",
    "    return beta_0, beta_1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_logit(train_x, train_y, test_x, test_y, iterations):\n",
    "    train_y[train_y==-1] = 0\n",
    "    test_y[test_y==-1] = 0\n",
    "    trainscore=[]\n",
    "    testscore=[]\n",
    "    h = np.zeros([train_x.shape[1],2])\n",
    "\n",
    "    for _ in range(iterations):\n",
    "        h_xi= np.sum(train_x * h[:,1] + h[:,0], axis=1)\n",
    "        #h_xi= predict(train_x , h[:,1] , h[:,0])\n",
    "        p=1/(1+np.exp(-2*h_xi))\n",
    "        w_i=(p)*(1-p)\n",
    "        z = (train_y - p)/(p*(1-p))\n",
    "        #print(\"weight:\", w_i.shape)\n",
    "        #w_i = np.ones_like(w_i)\n",
    "        print(\"w_i min\", np.min(w_i))\n",
    "        print(\"w_i max\", np.max(w_i))\n",
    "        Beta0, Beta1 = linear_regressor_for_each_feature(train_x, z, w_i)\n",
    "        selected_betas= beta_selection(train_x, Beta0, Beta1, train_y, h)\n",
    "\n",
    "        h += selected_betas\n",
    "        trainscore.append(1-accuracy_score(train_y, predict(train_x, h[:,1], h[:,0])))\n",
    "        testscore.append(1-accuracy_score(test_y, predict(test_x, h[:,1], h[:,0])))\n",
    "\n",
    "    #print(\"iteration\", iterations, \"num features\", np.sum(np.nonzero(h[:,0])))\n",
    "    #print(\"iteration\", iterations, \"h\", h)\n",
    "    return trainscore, testscore"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def predict(x, b1, b0):\n",
    "    prediction = np.sum(x*b1 + b0, axis=1)\n",
    "    prediction[prediction >= 0] = 1\n",
    "    prediction[prediction < 0] = 0\n",
    "    return prediction"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot(train_errors, test_errors, iteration, train_error_to_plot, test_error_to_plot):\n",
    "    i = list(range(len(test_error_to_plot)))\n",
    "    plt.plot(i, train_error_to_plot)\n",
    "    plt.plot(i, test_error_to_plot)\n",
    "    plt.legend([\"Train\", \"Test\"])\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Misclassification error\")\n",
    "    plt.title(\"Error vs iterations with 100 features\")\n",
    "\n",
    "    test_errors = [f\"{test_error:.3f}\" for test_error in test_errors]\n",
    "    train_errors = [f\"{train_error:.3f}\" for train_error in train_errors]\n",
    "    plt.table(cellText=[*zip(iteration, train_errors, test_errors)], colLabels=['Iteration', 'Training error', 'Test error'],\n",
    "              bbox=[0.0,-0.8, 1,.4], edges=\"closed\" )\n",
    "    plt.show()\n",
    "\n",
    "def plot_logit(xtrain, ytrain, xtest, ytest):\n",
    "    error_test=[]\n",
    "    error_train=[]\n",
    "\n",
    "    iterations=[100]\n",
    "    plot_train_error=[]\n",
    "    plot_test_error=[]\n",
    "\n",
    "    for x in iterations:\n",
    "        trainscore, testscore = train_logit(xtrain, ytrain, xtest, ytest, x)\n",
    "        error_train.append(trainscore[-1])\n",
    "        error_test.append(testscore[-1])\n",
    "        if x == 100 :\n",
    "            plot_train_error = trainscore\n",
    "            plot_test_error = testscore\n",
    "    plot(error_train, error_test, iterations, plot_train_error, plot_test_error)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Plot the error vs iteration and display final error in a table"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## a) Gisette\n",
    "\n",
    "Using the Gisette data, train a Logitboost classifier on the training set, with k ∈ {10, 30, 100, 300, 500} boosted iterations. Plot the training loss vs iteration number for k = 500. Report in a table the misclassification errors on the training and test set for the models obtained for all these k. Plot the misclassification errors on the training and test set vs k. (5 points)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#train_x, train_y, test_x, test_y = get_gisette()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#plot_logit(train_x, train_y, test_x, test_y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## b) Dexter\n",
    "Repeat point a) on the dexter dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#train_x, train_y, test_x, test_y = get_dexter()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#plot_logit(train_x, train_y, test_x, test_y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## c) Madelon\n",
    "Repeat point a) on the madelon dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "train_x, train_y, test_x, test_y = get_madelon()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_logit(train_x, train_y, test_x, test_y)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "Oscar Kosar-Kosarewicz"
   },
   {
    "name": "Nicholas Phillips"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}