{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Logitboost Assignment\n",
    "Implement Logitboost using 1D linear regressors as weak learners. At each boosting\n",
    "iteration choose the weak learner that obtains the largest reduction in the loss function\n",
    "on the training set $D = {(x_i, y_i), i = 1, ..., N}, \\text{ with } y_i ∈ {0, 1}$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data loading functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_gisette():\n",
    "    path = \"data/gisette/\"   \n",
    "    \n",
    "    train_x = np.loadtxt(path+\"gisette_train.data\")\n",
    "    train_y = np.loadtxt(path+\"gisette_train.labels\")\n",
    "    \n",
    "    valid_x = np.loadtxt(path+\"gisette_valid.data\")\n",
    "    valid_y = np.loadtxt(path+\"gisette_valid.labels\")\n",
    "    \n",
    "    return train_x, train_y, valid_x, valid_y\n",
    "\n",
    "\n",
    "\n",
    "def get_dexter():\n",
    "    path = \"data/dexter/\"\n",
    "\n",
    "    train_x = np.loadtxt(path+\"dexter_train.csv\", delimiter=',')\n",
    "    train_y = np.loadtxt(path+\"dexter_train.labels\")\n",
    "\n",
    "    valid_x = np.loadtxt(path+\"dexter_valid.csv\", delimiter=',')\n",
    "    valid_y = np.loadtxt(path+\"dexter_valid.labels\")\n",
    "\n",
    "    return train_x, train_y, valid_x, valid_y\n",
    "\n",
    "def get_madelon():\n",
    "    path = \"data/MADELON/\"\n",
    "\n",
    "    train_x = np.loadtxt(path + \"madelon_train.data\")\n",
    "    train_y = np.loadtxt(path + \"madelon_train.labels\")\n",
    "    test_x = np.loadtxt(path + \"madelon_valid.data\")\n",
    "    test_y = np.loadtxt(path + \"madelon_valid.labels\")\n",
    "\n",
    "    return train_x, train_y, test_x, test_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Normalization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def normalize(train, *args):\n",
    "    mean = np.average(train, axis=0)\n",
    "    standard_deviation = np.std(train, axis=0)\n",
    "    columns = train, *args\n",
    "    return tuple(np.divide(column-mean, standard_deviation, where=standard_deviation!=0)\n",
    "                 for column in columns)\n",
    "\n",
    "\n",
    "def beta_selection(x, beta0, beta1, y):\n",
    "    h_xi= x * beta1 + beta0\n",
    "    ytilde=2*y-1\n",
    "    loss=np.sum(np.log(1+np.exp(-ytilde*h_xi.T)), axis=1)\n",
    "    beta_index = np.argmin(loss)\n",
    "    filter = np.ones_like(beta0) # array of ones of same length as beta0\n",
    "    filter[beta_index] = 0 # set the spot at beta_index to 0\n",
    "    beta0[filter==1] = 0 # all indices at which filter==1 are set to 0, to beta 0 has zeroes everywhere except beta_index\n",
    "    beta1[filter==1] = 0\n",
    "    return np.column_stack(np.dstack((beta0,beta1)))\n",
    "\n",
    "\n",
    "\n",
    "#def h()\n",
    "# I separated linear regression to its own function and transposed to vectors so they\n",
    "# broadcast correctly\n",
    "def linear_regressor_for_each_feature(x, y, w):\n",
    "    mean_x = np.average(x, axis=0) # add an axis so we can tranpose this\n",
    "    mean_y = np.average(y)\n",
    "    denominators = np.sum(w*np.square(x-mean_x.T), axis=0)\n",
    "    beta_1= np.sum(w*(y-mean_y)[np.newaxis].T*(x-mean_x.T), axis=0)/ denominators\n",
    "    beta_0 = mean_y + beta_1 * mean_x\n",
    "    beta_0 = np.ravel(beta_0) # remove extra axis\n",
    "    beta_0[np.isnan(beta_0)] = 0\n",
    "    beta_1[np.isnan(beta_1)] = 0\n",
    "    return beta_0, beta_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "train_x, train_y, test_x, test_y = get_dexter()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicholasphillips/PycharmProjects/MachineLearningAssignments/venv/lib/python3.7/site-packages/ipykernel_launcher.py:29: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[0., 0.],\n       [0., 0.],\n       [0., 0.],\n       ...,\n       [0., 0.],\n       [0., 0.],\n       [0., 0.]])"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test function\n",
    "beta_selection(train_x, *linear_regressor_for_each_feature(train_x, train_y, np.ones(train_x.shape[1])), train_y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "def train_logit(x_train, y_train, x_test, y_test,):\n",
    "    interations=500\n",
    "\n",
    "    y_train[y_train==-1] = 0\n",
    "    y_test[y_test==-1] = 0\n",
    "    trainscore=[]\n",
    "    testscore=[]\n",
    "    h = np.zeros([x_train.shape[1],2])\n",
    "\n",
    "    for x in range(interations):\n",
    "        h_xi= x * h[:,1] + h[:,0]\n",
    "        p=1/(1+np.exp(-h_xi))\n",
    "        w_i=(p)*(1-p)\n",
    "        Beta0, Beta1 = linear_regressor_for_each_feature(x_train, y_train, w_i)\n",
    "        h += beta_selection(x_train, Beta0, Beta1, y_train)\n",
    "        trainscore.apend(1-accuracy_score(train_y, predict(train_x)))\n",
    "        testscore.append(1-accuracy_score(test_y, predict(test_x, Beta1, Beta0)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "def predict(x, b1, b0):\n",
    "    return x*b1 + b0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "def plot(features, test_errors, train_errors, train_errors_to_plot, test_errors_to_plot):\n",
    "    iteration = list(range(len(train_errors_to_plot)))\n",
    "    plt.plot(iteration, train_errors_to_plot);\n",
    "    plt.plot(iteration, test_errors_to_plot);\n",
    "    plt.legend([\"Train\", \"Test\"])\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Misclassification error\")\n",
    "    plt.title(\"Error vs iterations with 30 features\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(features, train_errors)\n",
    "    plt.plot(features, test_errors)\n",
    "    plt.legend([\"Train\", \"Test\"])\n",
    "    plt.xlabel(\"Number of features\")\n",
    "    plt.ylabel(\"Final misclassification error\")\n",
    "    plt.title(\"Features vs misclassification error\")\n",
    "\n",
    "    test_errors = [f\"{test_error:.3f}\" for test_error in test_errors]\n",
    "    train_errors = [f\"{train_error:.3f}\" for train_error in train_errors]\n",
    "    plt.table(cellText=[*zip(features, train_errors, test_errors)], colLabels=['Features', 'Training error', 'Test error'],\n",
    "              bbox=[0.0,-0.8, 1,.4], edges=\"closed\" )\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Plot the error vs iteration and display final error in a table"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## a) Gisette\n",
    "\n",
    "Using the Gisette data, train a FSA classifier on the training set, starting with β(0) = 0 to select k ∈ {10, 30, 100, 300, 500} features. Plot the training loss vs iteration number for k = 30. Report in a table the misclassification errors on the training and test set for the models obtained for all these k. Plot the misclassification error on the training and test set vs k."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "#train_x, train_y, test_x, test_y, _ = get_gisette()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "#calibrate_k(train_x, train_y, test_x, test_y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "#plot_fsa(train_x, train_y, test_x, test_y, [10, 30, 100, 300, 500]);"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## b) Dexter\n",
    "Repeat point a) on the dexter dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "#train_x, train_y, test_x, test_y = get_dexter()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "#linear_regressor_for_each_feature(train_x, train_y, np.ones(train_x.shape[1]))\n",
    "#calibrate_k(train_x, train_y, test_x, test_y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "#plot_fsa(train_x, train_y, test_x, test_y, [10, 30, 100, 300, 500]);"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## c) Madelon\n",
    "Repeat point a) on the madelon dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'apend'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-42-02bb9f862c85>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mtrain_x\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain_y\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtest_x\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtest_y\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mget_madelon\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mtrain_logit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_x\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain_y\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtest_x\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtest_y\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m<ipython-input-33-d388f93ac0b1>\u001B[0m in \u001B[0;36mtrain_logit\u001B[0;34m(x_train, y_train, x_test, y_test)\u001B[0m\n\u001B[1;32m     14\u001B[0m         \u001B[0mBeta0\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mBeta1\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlinear_regressor_for_each_feature\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx_train\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my_train\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mw_i\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     15\u001B[0m         \u001B[0mh\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0mbeta_selection\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx_train\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mBeta0\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mBeta1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my_train\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 16\u001B[0;31m         \u001B[0mtrainscore\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mapend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0maccuracy_score\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_y\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpredict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_x\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     17\u001B[0m         \u001B[0mtestscore\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0maccuracy_score\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtest_y\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpredict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtest_x\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     18\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'list' object has no attribute 'apend'"
     ]
    }
   ],
   "source": [
    "train_x, train_y, test_x, test_y = get_madelon()\n",
    "train_logit(train_x, train_y, test_x, test_y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#calibrate_k(train_x, train_y, test_x, test_y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "Oscar Kosar-Kosarewicz"
   },
   {
    "name": "Nicholas Phillips"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}