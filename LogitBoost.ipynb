{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Logitboost Assignment\n",
    "Implement Logitboost using 1D linear regressors as weak learners. At each boosting\n",
    "iteration choose the weak learner that obtains the largest reduction in the loss function\n",
    "on the training set $D = {(x_i, y_i), i = 1, ..., N}, \\text{ with } y_i ∈ {0, 1}$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data loading functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_gisette():\n",
    "    path = \"data/gisette/\"   \n",
    "    \n",
    "    train_x = np.loadtxt(path+\"gisette_train.data\")\n",
    "    train_y = np.loadtxt(path+\"gisette_train.labels\")\n",
    "    \n",
    "    valid_x = np.loadtxt(path+\"gisette_valid.data\")\n",
    "    valid_y = np.loadtxt(path+\"gisette_valid.labels\")\n",
    "    \n",
    "    test_x = np.loadtxt(path+\"gisette_test.data\")\n",
    "\n",
    "    return train_x, train_y, valid_x, valid_y, test_x\n",
    "\n",
    "\n",
    "\n",
    "def get_dexter():\n",
    "    path = \"data/dexter/\"\n",
    "\n",
    "    train_x = np.loadtxt(path+\"dexter_train.csv\", delimiter=',')\n",
    "    train_y = np.loadtxt(path+\"dexter_train.labels\")\n",
    "\n",
    "    valid_x = np.loadtxt(path+\"dexter_valid.csv\", delimiter=',')\n",
    "    valid_y = np.loadtxt(path+\"dexter_valid.labels\")\n",
    "\n",
    "    return train_x, train_y, valid_x, valid_y\n",
    "\n",
    "def get_madelon():\n",
    "    path = \"data/MADELON/\"\n",
    "\n",
    "    train_x = np.loadtxt(path + \"madelon_train.data\")\n",
    "    train_y = np.loadtxt(path + \"madelon_train.labels\")\n",
    "    test_x = np.loadtxt(path + \"madelon_valid.data\")\n",
    "    test_y = np.loadtxt(path + \"madelon_valid.labels\")\n",
    "\n",
    "    return train_x, train_y, test_x, test_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Normalization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def normalize(train, *args):\n",
    "    mean = np.average(train, axis=0)\n",
    "    standard_deviation = np.std(train, axis=0)\n",
    "    columns = train, *args\n",
    "    return tuple(np.divide(column-mean, standard_deviation, where=standard_deviation!=0)\n",
    "                 for column in columns)\n",
    "\n",
    "def hofx(train_x, train_y):\n",
    "    h=0\n",
    "\n",
    "    p=1/(1+np.exp(-h)) # h doesn't make sense\n",
    "    w_i=(p)*(1-p)\n",
    "    #mean_x = np.average(train_x, axis=0)\n",
    "    #mean_y = np.average(train_y, axis=0)\n",
    "    #beta_1= np.sum(w_i*(train_y-mean_y)*(train_x*mean_x))\n",
    "    #beta_1 /=(np.sum(w_i*np.square(train_x-mean_x)))\n",
    "    #beta_0= mean_y + beta_1 * mean_x\n",
    "    beta_0, beta_1 = linear_regressor_for_each_feature(train_x, train_y, w_i)\n",
    "\n",
    "\n",
    "def beta_selection(x, beta0, beta1, y):\n",
    "    h_xi= x * beta1 + beta0\n",
    "    ytilde=2*y-1\n",
    "    loss=np.sum(np.log(1+np.exp(-ytilde*h_xi.T)), axis=1)\n",
    "    beta_index = np.argmin(loss)\n",
    "    filter = np.ones_like(beta0) # array of ones of same length as beta0\n",
    "    filter[beta_index] = 0 # set the spot at beta_index to 0\n",
    "    beta0[filter==1] = 0 # all indices at which filter==1 are set to 0, to beta 0 has zeroes everywhere except beta_index\n",
    "    beta1[filter==1] = 0\n",
    "    return beta0[beta_index], beta1[beta_index]\n",
    "\n",
    "\n",
    "\n",
    "#def h()\n",
    "# I separated linear regression to its own function and transposed to vectors so they\n",
    "# broadcast correctly\n",
    "def linear_regressor_for_each_feature(x, y, w):\n",
    "    mean_x = np.average(x, axis=0) # add an axis so we can tranpose this\n",
    "    mean_y = np.average(y)\n",
    "    denominators = np.sum(w*np.square(x-mean_x.T), axis=0)\n",
    "    beta_1= np.divide(np.sum(w*(y-mean_y)[np.newaxis].T*(x-mean_x.T), axis=0), denominators, where=denominators!=0)\n",
    "    #print(np.sum(np.isnan(beta_1)))\n",
    "    beta_0= mean_y + beta_1 * mean_x\n",
    "    beta_0 = np.ravel(beta_0) # remove extra axis\n",
    "    #print(np.sum(np.isnan(beta_0)))\n",
    "    #print(np.sum(np.isnan(beta_1)))\n",
    "    return beta_0, beta_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "outputs": [],
   "source": [
    "train_x, train_y, test_x, test_y = get_madelon()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "outputs": [
    {
     "data": {
      "text/plain": "(-0.22536303482965425, -0.0004408130654436125)"
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test function\n",
    "beta_selection(train_x, *linear_regressor_for_each_feature(train_x, train_y, np.ones(train_x.shape[1])), train_y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Find k for 10, 30, 100, 300, 500 features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Plot the error vs iteration and display final error in a table"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## a) Gisette\n",
    "\n",
    "Using the Gisette data, train a FSA classifier on the training set, starting with β(0) = 0 to select k ∈ {10, 30, 100, 300, 500} features. Plot the training loss vs iteration number for k = 30. Report in a table the misclassification errors on the training and test set for the models obtained for all these k. Plot the misclassification error on the training and test set vs k."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "#train_x, train_y, test_x, test_y, _ = get_gisette()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "#calibrate_k(train_x, train_y, test_x, test_y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "#plot_fsa(train_x, train_y, test_x, test_y, [10, 30, 100, 300, 500]);"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## b) Dexter\n",
    "Repeat point a) on the dexter dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [],
   "source": [
    "#train_x, train_y, test_x, test_y = get_dexter()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "#linear_regressor_for_each_feature(train_x, train_y, np.ones(train_x.shape[1]))\n",
    "#calibrate_k(train_x, train_y, test_x, test_y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "#plot_fsa(train_x, train_y, test_x, test_y, [10, 30, 100, 300, 500]);"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## c) Madelon\n",
    "Repeat point a) on the madelon dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "#train_x, train_y, test_x, test_y = get_madelon()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [],
   "source": [
    "#calibrate_k(train_x, train_y, test_x, test_y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "Oscar Kosar-Kosarewicz"
   },
   {
    "name": "Nicholas Phillips"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}