{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Logitboost Assignment\n",
    "Implement Logitboost using 1D linear regressors as weak learners. At each boosting\n",
    "iteration choose the weak learner that obtains the largest reduction in the loss function\n",
    "on the training set $D = {(x_i, y_i), i = 1, ..., N}, \\text{ with } y_i ∈ {0, 1}$:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data loading functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_gisette():\n",
    "    path = \"data/gisette/\"   \n",
    "    \n",
    "    train_x = np.loadtxt(path+\"gisette_train.data\")\n",
    "    train_y = np.loadtxt(path+\"gisette_train.labels\")\n",
    "    \n",
    "    valid_x = np.loadtxt(path+\"gisette_valid.data\")\n",
    "    valid_y = np.loadtxt(path+\"gisette_valid.labels\")\n",
    "    \n",
    "    return train_x, train_y, valid_x, valid_y\n",
    "\n",
    "\n",
    "\n",
    "def get_dexter():\n",
    "    path = \"data/dexter/\"\n",
    "\n",
    "    train_x = np.loadtxt(path+\"dexter_train.csv\", delimiter=',')\n",
    "    train_y = np.loadtxt(path+\"dexter_train.labels\")\n",
    "\n",
    "    valid_x = np.loadtxt(path+\"dexter_valid.csv\", delimiter=',')\n",
    "    valid_y = np.loadtxt(path+\"dexter_valid.labels\")\n",
    "\n",
    "    return train_x, train_y, valid_x, valid_y\n",
    "\n",
    "def get_madelon():\n",
    "    path = \"data/MADELON/\"\n",
    "\n",
    "    train_x = np.loadtxt(path + \"madelon_train.data\")\n",
    "    train_y = np.loadtxt(path + \"madelon_train.labels\")\n",
    "    test_x = np.loadtxt(path + \"madelon_valid.data\")\n",
    "    test_y = np.loadtxt(path + \"madelon_valid.labels\")\n",
    "\n",
    "    return train_x, train_y, test_x, test_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Normalization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def normalize(train, *args):\n",
    "    mean = np.average(train, axis=0)\n",
    "    standard_deviation = np.std(train, axis=0)\n",
    "    columns = train, *args\n",
    "    return tuple(np.divide(column-mean, standard_deviation, where=standard_deviation!=0)\n",
    "                 for column in columns)\n",
    "\n",
    "\n",
    "def beta_selection(x, beta0, beta1, y):\n",
    "    h_xi= x * beta1 + beta0\n",
    "    ytilde=2*y-1\n",
    "    loss=np.sum(np.log(1+np.exp(-ytilde*h_xi.T)), axis=1)\n",
    "    beta_index = np.argmin(loss)\n",
    "    filter = np.ones_like(beta0) # array of ones of same length as beta0\n",
    "    filter[beta_index] = 0 # set the spot at beta_index to 0\n",
    "    beta0[filter==1] = 0 # all indices at which filter==1 are set to 0, to beta 0 has zeroes everywhere except beta_index\n",
    "    beta1[filter==1] = 0\n",
    "    return np.column_stack(np.dstack((beta0,beta1)))\n",
    "\n",
    "\n",
    "\n",
    "#def h()\n",
    "# I separated linear regression to its own function and transposed to vectors so they\n",
    "# broadcast correctly\n",
    "def linear_regressor_for_each_feature(x, y, w):\n",
    "    mean_x = np.average(x, axis=0) # add an axis so we can tranpose this\n",
    "    mean_y = np.average(y)\n",
    "    denominators = np.sum(w*np.square(x-mean_x.T), axis=0)\n",
    "    beta_1= np.sum(w*(y-mean_y)[np.newaxis].T*(x-mean_x.T), axis=0)/ denominators\n",
    "    beta_0 = mean_y + beta_1 * mean_x\n",
    "    beta_0 = np.ravel(beta_0) # remove extra axis\n",
    "    beta_0[np.isnan(beta_0)] = 0\n",
    "    beta_1[np.isnan(beta_1)] = 0\n",
    "    return beta_0, beta_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "train_x, train_y, test_x, test_y = get_dexter()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicholasphillips/PycharmProjects/MachineLearningAssignments/venv/lib/python3.7/site-packages/ipykernel_launcher.py:29: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[0., 0.],\n       [0., 0.],\n       [0., 0.],\n       ...,\n       [0., 0.],\n       [0., 0.],\n       [0., 0.]])"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test function\n",
    "beta_selection(train_x, *linear_regressor_for_each_feature(train_x, train_y, np.ones(train_x.shape[1])), train_y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "def train_logit(x_train, y_train, x_test, y_test, i):\n",
    "    interations=i\n",
    "\n",
    "    y_train[y_train==-1] = 0\n",
    "    y_test[y_test==-1] = 0\n",
    "    trainscore=[]\n",
    "    testscore=[]\n",
    "    h = np.zeros([x_train.shape[1],2])\n",
    "\n",
    "    for x in range(interations):\n",
    "        h_xi= x * h[:,1] + h[:,0]\n",
    "        p=1/(1+np.exp(-h_xi))\n",
    "        w_i=(p)*(1-p)\n",
    "        Beta0, Beta1 = linear_regressor_for_each_feature(x_train, y_train, w_i)\n",
    "        h += beta_selection(x_train, Beta0, Beta1, y_train)\n",
    "        trainscore.append(1-accuracy_score(train_y, predict(train_x, h[:,1], h[:,0])))\n",
    "        testscore.append(1-accuracy_score(test_y, predict(test_x, h[:,1], h[:,0])))\n",
    "    return trainscore, testscore"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "def predict(x, b1, b0):\n",
    "    prediction = np.sum(x*b1 + b0, axis=1)\n",
    "    prediction[prediction >= .5] = 1\n",
    "    prediction[prediction < .5] = 0\n",
    "    return prediction"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "def plot(test_errors, train_errors, iteration):\n",
    "    iteration = list(range(len(train_errors)))\n",
    "    plt.plot(iteration, train_errors);\n",
    "    plt.plot(iteration, test_errors);\n",
    "    plt.legend([\"Train\", \"Test\"])\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Misclassification error\")\n",
    "    plt.title(\"Error vs iterations with 30 features\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    test_errors = [f\"{test_error:.3f}\" for test_error in test_errors]\n",
    "    train_errors = [f\"{train_error:.3f}\" for train_error in train_errors]\n",
    "    plt.table(cellText=[*zip(iteration, train_errors, test_errors)], colLabels=['Iteration', 'Training error', 'Test error'],\n",
    "              bbox=[0.0,-0.8, 1,.4], edges=\"closed\" )\n",
    "    plt.show()\n",
    "def plot_logit(xtrain, ytrain, xtest, ytest):\n",
    "    error_test=[]\n",
    "    error_train=[]\n",
    "\n",
    "    iterations=[10, 30, 100, 300, 500]\n",
    "\n",
    "    for x in iterations:\n",
    "        error_train.append(train_logit(xtrain, ytrain, xtest, ytest, x).trainscore)\n",
    "        error_test.append(train_logit(xtrain, ytrain, xtest, ytest, x).testscore)\n",
    "    plot(error_train, error_test, iterations)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Plot the error vs iteration and display final error in a table"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## a) Gisette\n",
    "\n",
    "Using the Gisette data, train a FSA classifier on the training set, starting with β(0) = 0 to select k ∈ {10, 30, 100, 300, 500} features. Plot the training loss vs iteration number for k = 30. Report in a table the misclassification errors on the training and test set for the models obtained for all these k. Plot the misclassification error on the training and test set vs k."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "#train_x, train_y, test_x, test_y, _ = get_gisette()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "#calibrate_k(train_x, train_y, test_x, test_y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "#plot_fsa(train_x, train_y, test_x, test_y, [10, 30, 100, 300, 500]);"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## b) Dexter\n",
    "Repeat point a) on the dexter dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "#train_x, train_y, test_x, test_y = get_dexter()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "#linear_regressor_for_each_feature(train_x, train_y, np.ones(train_x.shape[1]))\n",
    "#calibrate_k(train_x, train_y, test_x, test_y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "#plot(train_x, train_y, test_x, test_y);"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## c) Madelon\n",
    "Repeat point a) on the madelon dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "train_logit() missing 1 required positional argument: 'i'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-46-3b502921b898>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mtrain_x\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain_y\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtest_x\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtest_y\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mget_madelon\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mtrain_logit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_x\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain_y\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtest_x\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtest_y\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: train_logit() missing 1 required positional argument: 'i'"
     ]
    }
   ],
   "source": [
    "train_x, train_y, test_x, test_y = get_madelon()\n",
    "#train_logit(train_x, train_y, test_x, test_y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_logit(train_x, train_y, test_x, test_y)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "Oscar Kosar-Kosarewicz"
   },
   {
    "name": "Nicholas Phillips"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}